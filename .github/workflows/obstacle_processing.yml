name: supabase-batch-infer-every-15m

on:
  schedule:
    - cron: "*/15 * * * *"
  workflow_dispatch: {}

concurrency:
  group: supabase-batch-infer
  cancel-in-progress: false

jobs:
  run-inference:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
      MODEL_PATH: backend/Models/FootPathDetector/segformer_fp.onnx
      TABLE_IN: blocked_image
      COL_NODE_ID: node_id
      COL_CREATED_ON: created_on
      COL_FILEPATH: filepath
      TABLE_OUT: blocked_result
      MAX_FILES_PER_RUN: "10"
      INPUT_SIZE: "384"
      CONF_THRESHOLD: "0.5"

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: "requirements.txt"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Verify model presence
        run: |
          test -f "$MODEL_PATH" || { echo "::error::Model not found at $MODEL_PATH"; exit 1; }
          ls -lh "$MODEL_PATH"

      - name: Run batch inference
        run: |
          python - <<'PY'
          import os, io, time, json, base64, hashlib
          from pathlib import Path
          import numpy as np
          from PIL import Image
          import onnxruntime as ort
          import requests
          from supabase import create_client

          SUPABASE_URL = os.environ["SUPABASE_URL"]
          SUPABASE_SERVICE_KEY = os.environ["SUPABASE_SERVICE_KEY"]
          TABLE_IN = os.environ["TABLE_IN"]
          COL_NODE_ID = os.environ["COL_NODE_ID"]
          COL_CREATED_ON = os.environ["COL_CREATED_ON"]
          COL_FILEPATH = os.environ["COL_FILEPATH"]
          TABLE_OUT = os.environ["TABLE_OUT"]
          MODEL_PATH = os.environ["MODEL_PATH"]
          MAX_FILES = int(os.environ.get("MAX_FILES_PER_RUN","10"))
          INPUT_SIZE = int(os.environ.get("INPUT_SIZE","384"))
          CONF_THR = float(os.environ.get("CONF_THRESHOLD","0.5"))

          Path("batch_logs").mkdir(exist_ok=True)
          log_fp = Path("batch_logs") / f"run_{int(time.time())}.txt"
          def log(m): print(m, flush=True); open(log_fp,"a",encoding="utf-8").write(m+"\n")

          def letterbox(img, size):
              img = img.convert("RGB")
              W,H = img.size
              s = size / max(W,H)
              nW,nH = int(round(W*s)), int(round(H*s))
              r = img.resize((nW,nH), Image.BILINEAR)
              c = Image.new("RGB",(size,size),(0,0,0))
              c.paste(r, ((size-nW)//2,(size-nH)//2))
              a = np.asarray(c,dtype=np.float32)/255.0
              mean = np.array([0.485,0.456,0.406],dtype=np.float32)
              std  = np.array([0.229,0.224,0.225],dtype=np.float32)
              a = (a-mean)/std
              return a.transpose(2,0,1)[None,...]

          def mask_from_logits(logits, out_size):
              if logits.shape[1]==1:
                  probs = 1/(1+np.exp(-logits))
                  m = (probs[0,0] >= CONF_THR).astype(np.uint8)*255
              else:
                  cls = np.argmax(logits, axis=1)
                  m = (cls[0]==1).astype(np.uint8)*255
              im = Image.fromarray(m).resize((out_size,out_size), Image.NEAREST)
              return im

          supa = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
          db = supa.table(TABLE_IN)
          out_tbl = supa.table(TABLE_OUT)

          so = ort.SessionOptions()
          so.intra_op_num_threads = 2
          so.inter_op_num_threads = 1
          session = ort.InferenceSession(MODEL_PATH, providers=["CPUExecutionProvider"], sess_options=so)
          in_name = session.get_inputs()[0].name
          out_name = session.get_outputs()[0].name

          q = db.select("*").order(COL_CREATED_ON, desc=False).limit(MAX_FILES)
          rows = q.execute().data or []
          if not rows:
              log("No rows to process.")
              print("Done.")
              raise SystemExit(0)

          def key_for(row):
              nid = str(row.get(COL_NODE_ID))
              con = str(row.get(COL_CREATED_ON))
              fp  = str(row.get(COL_FILEPATH))
              raw = f"{nid}|{con}|{fp}".encode("utf-8")
              return hashlib.sha1(raw).hexdigest()

          def already_done(k):
              try:
                  r = out_tbl.select("image_key").eq("image_key", k).limit(1).execute()
                  return bool(r.data)
              except Exception:
                  return False

          processed = 0
          for row in rows:
              nid = row.get(COL_NODE_ID)
              con = row.get(COL_CREATED_ON)
              fp  = row.get(COL_FILEPATH)
              if not fp:
                  log("SKIP row without filepath")
                  continue
              k = key_for(row)
              if already_done(k):
                  log(f"SKIP {k} (already processed)")
                  continue
              try:
                  r = requests.get(str(fp), timeout=30)
                  r.raise_for_status()
                  img = Image.open(io.BytesIO(r.content)).convert("RGB")
                  xin = letterbox(img, INPUT_SIZE)
                  t1 = time.time()
                  logits = session.run([out_name], {in_name: xin})[0]
                  t2 = time.time()
                  mask_img = mask_from_logits(logits, INPUT_SIZE)
                  buf = io.BytesIO()
                  mask_img.save(buf, format="PNG")
                  mask_b64 = base64.b64encode(buf.getvalue()).decode("ascii")
                  meta = {
                      "node_id": nid,
                      "created_on": str(con),
                      "filepath": fp,
                      "model": Path(MODEL_PATH).name,
                      "input_size": INPUT_SIZE,
                      "runtime_ms": int((t2-t1)*1000),
                      "ts": int(time.time())
                  }
                  out_tbl.upsert({
                      "image_key": k,
                      "node_id": nid,
                      "created_on": con,
                      "filepath": fp,
                      "mask_b64": mask_b64,
                      "meta": meta
                  }).execute()
                  log(f"OK {k} ({meta['runtime_ms']} ms)")
                  processed += 1
              except Exception as e:
                  log(f"ERROR {k}: {e}")
          print(f"Done. Processed {processed}/{len(rows)}.")
          PY

      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: supabase-batch-logs
          path: batch_logs/*.txt
          if-no-files-found: ignore
